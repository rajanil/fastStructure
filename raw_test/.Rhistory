q.z <- -2.1
#q.yz <- 3.1
q.yz <- 0.2
probs <- GetProbs(q.y=q.y, q.z=q.z, q.yz=q.yz)
n <- 50000
data <- GenerateData(n=n, q.y=q.y, q.z=q.z, q.yz=q.yz)
mean(data$y)
probs$p.y
mean(data$z)
probs$p.z
mean(data$z * data$y)
probs$p.yz
m.true <- c(probs$p.y, probs$p.z, probs$p.yz)
cor(data$y, data$z)
vb.fit <- VariationalFit(m.y=probs$p.y, m.z=probs$p.z, q.y=q.y, q.z=q.z, q.yz=q.yz)
vb.m.y <- vb.fit$m.y
vb.m.z <- vb.fit$m.z
# The true covariance
sigmasq.y <- probs$p.y * (1 - probs$p.y)
sigmasq.z <- probs$p.z * (1 - probs$p.z)
sigmasq.zy <- probs$p.yz
sigma.true <- matrix(c(sigmasq.y, sigmasq.zy, sigmasq.zy, sigmasq.z),
nrow=2, ncol=2)
# For what follows, see generalized_linear_response/notes.lyx.
Sigma <- function(m.y, m.z) {
sigma <- matrix(0, 2, 2)
diag(sigma) <- c(m.y * (1 - m.y), m.z * (1 - m.z))
return(sigma)
}
SMat <- function(m.y, m.z) {
s <- matrix(c(1,   0,
0,   1,
m.z, m.y),
nrow=3, ncol=2, byrow=TRUE)
return(s)
}
GetDaDt <- function(m.y, m.z) {
m <- c(m.y, m.z, m.y * m.z)
sigma <- Sigma(m.y, m.z)
full.sigma <- matrix(0, nrow=3, ncol=3)
full.sigma[1:2, 1:2] <- sigma
s <- SMat(m.y, m.z)
h <- matrix(c(0,    q.yz, 0,
q.yz, 0,    0), nrow=2, ncol=3, byrow=TRUE)
response <- s %*% sigma %*% h
vb.partial <- s %*% sigma %*% t(s)
dm.dt <- solve(diag(3) - response, vb.partial)
# Look at the variances.
sqrt(diag(dm.dt))
sqrt(diag(sigma))
sqrt(diag(sigma.true))
dm.dt
sigma
sigma.true
# Get the derivative of the variational log partition function.
da.dm <- c(solve(sigma, c(m.y, m.z)), 0)
# Look, to the extent that this is dm.dt * vb.sigma, this is
# necessarily a runaway process.
da.dt <- dm.dt %*% da.dm
return(da.dt)
}
m.y <- vb.m.y
m.z <- vb.m.z
weight <- 0.1
for (iter in 1:10) {
da.dt <- GetDaDt(m.y, m.z)
new.m.y <- weight * da.dt[1] + (1 - weight) * m.y
new.m.z <- weight * da.dt[2] + (1 - weight) * m.z
print(da.dt)
m.true
m.y <- new.m.y
m.z <- new.m.z
}
da.dt
m.true
c(m.true, da.dt
)
c(m.true, da.dt)
cbind(m.true, da.dt)
m.y <- vb.m.y
m.z <- vb.m.z
weight <- 0.1
for (iter in 1:10) {
da.dt <- GetDaDt(m.y, m.z)
new.m.y <- weight * da.dt[1] + (1 - weight) * m.y
new.m.z <- weight * da.dt[2] + (1 - weight) * m.z
print(da.dt)
#m.true
m.y <- new.m.y
m.z <- new.m.z
}
m.y <- vb.m.y
m.z <- vb.m.z
weight <- 0.5
for (iter in 1:10) {
da.dt <- GetDaDt(m.y, m.z)
new.m.y <- weight * da.dt[1] + (1 - weight) * m.y
new.m.z <- weight * da.dt[2] + (1 - weight) * m.z
print(da.dt)
#m.true
m.y <- new.m.y
m.z <- new.m.z
}
GetDaDt(m.true[1], m.true[2])
cbind(m.true, da.dt)
m.y <- vb.m.y
m.z <- vb.m.z
weight <- 1
for (iter in 1:10) {
da.dt <- GetDaDt(m.y, m.z)
new.m.y <- weight * da.dt[1] + (1 - weight) * m.y
new.m.z <- weight * da.dt[2] + (1 - weight) * m.z
print(da.dt)
#m.true
m.y <- new.m.y
m.z <- new.m.z
}
cbind(m.true, da.dt)
m.y <- vb.m.y
m.z <- vb.m.z
weight <- 1
for (iter in 1:100) {
da.dt <- GetDaDt(m.y, m.z)
new.m.y <- weight * da.dt[1] + (1 - weight) * m.y
new.m.z <- weight * da.dt[2] + (1 - weight) * m.z
print(da.dt)
#m.true
m.y <- new.m.y
m.z <- new.m.z
}
cbind(m.true, da.dt)
require(RhpcBLASctl)
require(parallel) # one of the core R packages
require(doParallel)
#>> Please remove packages you don't use.
# require(multicore); require(doMC) # alternative to parallel/doParallel
# require(Rmpi); require(doMPI) # to use Rmpi as the back-end
library(foreach)
library(iterators)
library(cluster)
library(rlecuyer)
install.packages("rlecuyer")
library(rlecuyer)
?pam
installed.packages()
?vector
vector(10)
vector("numeric", 10)
vector()
?subsample
?NROW
List1 <- sample(1:10, 100, replace=T)
List2 <- sample(1:10, 100, replace=T)
for(p in 1:NROW(List1)){
#print(p)
#>> You can use == here, not %in%, right?  Depending on how smart R is behind the
# scenes, that could be faster.
IndiciesMatching1=which(List1 %in% List1[p]) #List of indicies that match pth entry of first list
IndiciesMatching2=which(List2 %in% List2[p]) #List of indicies that match pth entry of second list
numberOfPositiveMatches=NROW(intersect(IndiciesMatching1,IndiciesMatching2)) #Times when Cij=1 for both given lists.
numberOfNegativeMatches=NROW(List1)-(NROW(IndiciesMatching1)+NROW(IndiciesMatching2)-numberOfPositiveMatches) #Times when Cij=0 for both given lists
#This is basic set-theory. A non-match of cij1 and cij2 can must have at least 1 TRUE. And then just make sure not to count instances where True occured twice and you're good.
Matches=Matches+numberOfPositiveMatches+numberOfNegativeMatches
Negatives=Negatives+NROW(List1)-(numberOfPositiveMatches+numberOfNegativeMatches) #Times when the values for Cij do not agree on our lists.
}
Matches <- 0
Negative <- 0
p <- 1
IndiciesMatching1=which(List1 %in% List1[p]) #List of indicies that match pth entry of first list
IndiciesMatching2=which(List2 %in% List2[p]) #List of indicies that match pth entry of second list
numberOfPositiveMatches=NROW(intersect(IndiciesMatching1,IndiciesMatching2)) #Times when Cij=1 for both given lists.
numberOfNegativeMatches=NROW(List1)-(NROW(IndiciesMatching1)+NROW(IndiciesMatching2)-numberOfPositiveMatches) #Times when Cij=0 for both given lists
sum((List1 == List1[1]) * (List2 == List1[p]))
numberOfPositiveMatches
IndiciesMatching1
IndiciesMatching2
NROW(intersect(IndiciesMatching1,IndiciesMatching2))
sum((List1 == List1[1]) * (List2 == List2[p]))
TRUE * FALSE
?microbenchmark
library(microbenchmark)
install.packages("microbenchmark")
?microbenchmark
library(microbenchmark)
?microbenchmark
tabc1c2colsums
??tabc1c2colsums
require(profr)
install.packages("profr")
install.packages("tiff")
install.packages("devtools")
install.packages("RCurl")
install.packages("devtools")
install.packages("devtools")
library(devtools)
install_url("http://cran.r-project.org/bin/macosx/contrib/3.1/tiff_0.1-5.tgz")
install.packages("EBTools")
setwd("/home/rgiordan/Documents/git_repos/fastStructure/raw_test")
d <- read.delim("testdata.ped", sep="")
d <- read.delim("testdata.ped", sep="", header=F)
head(d)
sum(is.na(d))
names(d)[1:6] <- c("family", "id", "pat.id", "mat.id", "sex", "phenotype")
head(d)
table(d$id)
table(d$pat.id)
table(d$mat.id)
intersect(d$mat.id, d$id)
table(d[,7])
table(d[,8])
table(d[,9])
table(d[,10])
table(d[,11])
history()
hist(Inf)
history(Inf)
d.map <- read.delim("testdata.map", sep="", header=F)
head(d.map)
names(d.map) <- c("chromosome", "snp", "distance", "position")
head(d.map)
nrow(d.map)
nrow(d)
table(d.map$chromosome)
ncol(d)
length(names(d)) - 6
snp.n <- (length(names(d)) - 6) / 2
snp.n <- (length(names(d)) - 6) / 2
if (snp.n %% 1 != 0) {
stop("Wrong number of columns in the ped file")
}
paste(c("a", "b"), 1:10)
# Name the allele columns
names(d[-(1:6)]) <- paste(c("a", "b"), rep(snp.n, each=2), sep=".")
head(d)
names(d)[-(1:6)] <- paste(c("a", "b"), rep(snp.n, each=2), sep=".")
head(d)
names(d)[-(1:6)] <- paste(c("a", "b"), rep(1:snp.n, each=2), sep=".")
head(d)
xi <- read.delim("testoutput_simple.3.xi", sep="", header=F)
dim(x1)
dim(xi)
nrow(d)
mean.p <- read.delim("testoutput_simple.3.meanP", sep="", header=F)
head(mean.p)
head(xi)
mean.p <- read.delim("testoutput_simple.3.meanP", sep="", header=F)
mean.q <- read.delim("testoutput_simple.3.meanQ", sep="", header=F)
dim(mean.p)
dim(mean.q)
rowSums(mean.q)
rowSums(mean.p)
head(mean.p)
head(d)
library(reshape2)
d.melt <- melt(d)
head(d)
head(d.melt)
id.cols <- c("family", "id", "pat.id", "mat.id", "sex", "phenotype")
d.melt <- melt(d, id.vars=id.cols)
head(d.melt)
d$location <- sub("[ab].", "", d$variable)
d.melt$location <- sub("[ab].", "", d.melt$variable)
head(d.melt)
d.melt$gene <- sub(".[0-9]*", "", d.melt$variable)
head(d.melt)
d.melt$location <- sub("[ab]\\.", "", d.melt$variable)
d.melt$gene <- sub("\\.[0-9]*", "", d.melt$variable)
head(d.melt)
d.shape <- dcast(d.melt, id + location ~ gene)
head(d.shape)
d.shape$g <- (d$a == 2) + (d$b == 2)
d.shape$g <- (d.shape$a == 2) + (d.shape$b == 2)
head(d.shape)
library(dplyr)
library(ggplot2)
working.directory <- file.path(Sys.getenv("GIT_REPO_LOC"),
"STAT215A_Fall2013/gsi_lab5")
images <- list()
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images[[file.num]] <- read.csv(sprintf("image%d.txt", file.num), header=TRUE)
}
images <- list()
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images[[file.num]] <- read.csv(file.path(working.directory,
sprintf("image%d.txt", file.num), header=TRUE))
}
file.num
sprintf("image%d.txt", file.num)
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images[[file.num]] <- read.csv(file.path(working.directory,
sprintf("image%d.txt", file.num)), header=TRUE)
}
head(images[[1]])
images <- list()
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images[[file.num]] <- read.csv(file.path(working.directory,
sprintf("image%d.txt", file.num)), header=FALSE)
names(images[[file.num]]) <- collabs
images[[file.num]]$image <- file.num
}
collabs
names(images[[file.num]])
images <- list()
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images[[file.num]] <- read.delim(file.path(working.directory,
sprintf("image%d.txt", file.num)),
header=FALSE, sep="")
names(images[[file.num]]) <- collabs
images[[file.num]]$image <- file.num
}
images.combined <- do.call(rbind, images)
rm(images)
head(images.combined)
images.list <- list()
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images.list[[file.num]] <- read.delim(file.path(working.directory,
sprintf("image%d.txt", file.num)),
header=FALSE, sep="")
names(images.list[[file.num]]) <- collabs
images.list[[file.num]]$image <- file.num
}
images <- do.call(rbind, images.list)
rm(images.list)
group_by(images, x, y, image) %>% summarize(min(x), min(y), max(x), max(y))
group_by(images, image) %>% summarize(min(x), min(y), max(x), max(y))
group_by(images, image) %>% summarize(min(x), max(x), min(y), max(y))
ranges <- group_by(images, image) %>% summarize(min(x), max(x), min(y), max(y))
x.min <- min(ranges$min(x))
ranges <- group_by(images, image) %>% summarize(min.x=min(x), max.x=max(x),
min.y=min(y), max.y=max(y))
x.min <- min(ranges$min.x)
x.min <- min(ranges$min.x)
x.max <- max(ranges$max.x)
y.min <- min(ranges$min.y)
y.max <- max(ranges$max.y)
x.max - x.min
images$x.box <- with(images, (x - min.x) %% 10)
images$x.box <- with(images, (x - x.min) %% 10)
box.size <- 10
images$x.box <- with(images, (x - x.min) %% box.size)
images$y.box <- with(images, (y - y.min) %% box.size)
head(images)
images$x.box <- with(images, floor((x - x.min) / box.size))
images$y.box <- with(images, floor((y - y.min) %% box.size))
ggplot(images) +
geom_point(aes(x=x, y=y, color=x.box)) + facet_grid(~ image)
ggplot(images) + geom_point(aes(x=x, y=y, color=factor(x.box))) + facet_grid(~ image)
ggplot(images) + geom_point(aes(x=x, y=y, color=factor(y.box))) + facet_grid(~ image)
images$y.box <- with(images, floor((y - y.min) / box.size))
ggplot(images) + geom_point(aes(x=x, y=y, color=factor(y.box))) + facet_grid(~ image)
?summarize
?summarise
group(images, x.box, y.box) %>% summarize(count=n())
group_by(images, x.box, y.box) %>% summarize(count=n())
images$x.box <- with(images, floor((x - x.min) / box.size))
images$y.box <- with(images, floor((y - y.min) / box.size))
group_by(images, x.box, y.box) %>% summarize(count=n())
select(images, x.box==0, y.box ==0)
filter(images, x.box==0, y.box ==0)
filter(images, x.box==0, y.box ==0) %>% select(x, y, image)
filter(images, x.box==0, y.box ==0) %>% select(x, y, image, AF)
group_by(images, x.box, y.box) %>% summarize()
group_by(images, x.box, y.box, image) %>% summarize()
group_by(images, x.box, y.box, image) %>% summarize() %>% summarize(n())
boxes <- group_by(images, x.box, y.box, image) %>% summarize()
boxes %>% summarize(n()) %>% ungroup() %>% summarize()
boxes %>% summarize(n()) %>% ungroup()
boxes %>% summarize(n=n()) %>% ungroup() %>% group_by(n) %>% unique()
boxes %>% summarize(n=n()) %>% ungroup() %>% group_by(n) %>% summarize(n())
boxes <- group_by(images, x.box, y.box, image) %>% summarize()
boxes %>% summarize(n=n()) %>% ungroup() %>% group_by(n) %>% summarize(n())
sample(1:3)
sample(1:3)
sample(1:3)
group_by(boxes, x.box, y.box) %>% mutate(new.image=sample(1:3))
images <- inner_joint(images, boxes, by=c("x.box", "y.box", "image"))
images <- inner_join(images, boxes, by=c("x.box", "y.box", "image"))
head(images)
boxes <- group_by(boxes, x.box, y.box) %>% mutate(new.image=sample(1:3))
images <- inner_join(images, boxes, by=c("x.box", "y.box", "image"))
head(images)
library(grid.arrange)
??grid.arrange
library(gridExrra)
library(gridExtra)
gridArrange(
ggplot(filter(images, image == 1)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == 1)) + geom_point(aes(x=x, y=y, color=AF))
)
grid.arrange(
ggplot(filter(images, image == 1)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == 1)) + geom_point(aes(x=x, y=y, color=AF))
)
group_by(images, new.image, x, y) %>% summarize(n=n()) %>%
ungroup() %>% group_by(n) %>% summarize(count=n())
boxes %>% summarize(n=n()) %>% ungroup() %>% group_by(n) %>% summarize(n())
image.choice <- 2
grid.arrange(
ggplot(filter(images, image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == image.choice)) + geom_point(aes(x=x, y=y, color=AF))
)
?grid.arrange
grid.arrange(
ggplot(filter(images, image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
nrow=1
)
image.choice <- 3
grid.arrange(
ggplot(filter(images, image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
nrow=1
)
ggplot(images) + geom_point(aes(x=x,y=y,color=AF)) + facet_grid(. ~ image)
?write.table
for (file.num in 1:3) {
cat("Writing image ", file.num, "\n")
write.table(select(images, new.image == file.num),
file=file.path(working.directory, sprintf("new_image%d.txt", file.num)),
quote=FALSE, header=FALSE)
}
args(write.table)
for (file.num in 1:3) {
cat("Writing image ", file.num, "\n")
write.table(select(images, new.image == file.num)[collabs],
file=file.path(working.directory, sprintf("new_image%d.txt", file.num)),
quote=FALSE, row.names=FALSE, col.names=FALSE)
}
select(images, new.image == file.num)[collabs]
select(images, new.image == file.num)
file.num
head(images)
unique(images$new.image)
for (file.num in 1:3) {
cat("Writing image ", file.num, "\n")
write.table(filter(images, new.image == file.num)[collabs],
file=file.path(working.directory, sprintf("new_image%d.txt", file.num)),
quote=FALSE, row.names=FALSE, col.names=FALSE)
}
run.sanity.checks <- FALSE
# I'll set the seed so that I can re-generate exactly the same files if
# need be.
set.seed(42)
images.list <- list()
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
for (file.num in 1:3) {
cat("Loading image ", file.num, "\n")
images.list[[file.num]] <- read.delim(file.path(working.directory,
sprintf("image%d.txt", file.num)),
header=FALSE, sep="")
names(images.list[[file.num]]) <- collabs
images.list[[file.num]]$image <- file.num
}
images <- do.call(rbind, images.list)
rm(images.list)
# Look to confirm that the x and y ranges are similar, and then define
# the limits for the whole image.
ranges <- group_by(images, image) %>% summarize(min.x=min(x), max.x=max(x),
min.y=min(y), max.y=max(y))
x.min <- min(ranges$min.x)
y.min <- min(ranges$min.y)
# Break the rows into boxes.
box.size <- 10
images$x.box <- with(images, floor((x - x.min) / box.size))
images$y.box <- with(images, floor((y - y.min) / box.size))
# Look to make sure I did it right.  The first graph should have vertical
# stripes, and the second should have horizontal stripes.
if (run.sanity.checks) {
ggplot(images) + geom_point(aes(x=x, y=y, color=factor(x.box))) + facet_grid(~ image)
ggplot(images) + geom_point(aes(x=x, y=y, color=factor(y.box))) + facet_grid(~ image)
}
boxes <- group_by(images, x.box, y.box, image) %>% summarize()
if (run.sanity.checks) {
# Make sure each box occurs exactly three times.
boxes %>% summarize(n=n()) %>% ungroup() %>% group_by(n) %>% summarize(n())
}
# Shuffle the images within each box and merge with the original frame.
boxes <- group_by(boxes, x.box, y.box) %>% mutate(new.image=sample(1:3))
images <- inner_join(images, boxes, by=c("x.box", "y.box", "image"))
if (run.sanity.checks) {
# Make sure there's still only one pixel at each point per image.
group_by(images, new.image, x, y) %>% summarize(n=n()) %>%
ungroup() %>% group_by(n) %>% summarize(count=n())
# Take a look at the result for one image.
image.choice <- 1
grid.arrange(
ggplot(filter(images, image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
nrow=1
)
}
for (file.num in 1:3) {
cat("Writing image ", file.num, "\n")
write.table(filter(images, new.image == file.num)[collabs],
file=file.path(working.directory, sprintf("new_image%d.txt", file.num)),
quote=FALSE, row.names=FALSE, col.names=FALSE)
}
image.choice <- 1
grid.arrange(
ggplot(filter(images, image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
ggplot(filter(images, new.image == image.choice)) + geom_point(aes(x=x, y=y, color=AF)),
nrow=1
)
